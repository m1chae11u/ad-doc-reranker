{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chloe\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List, Optional\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from datasets import Features, Sequence, Value, load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    DPRContextEncoder,\n",
    "    DPRContextEncoderTokenizerFast,\n",
    "    HfArgumentParser,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    RagTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def split_text(text: str, n=100, character=\" \") -> List[str]:\n",
    "    \"\"\"Split the text every ``n``-th occurrence of ``character``\"\"\"\n",
    "    text = text.split(character)\n",
    "    return [character.join(text[i : i + n]).strip() for i in range(0, len(text), n)]\n",
    "\n",
    "\n",
    "def split_documents(documents: dict) -> dict:\n",
    "    \"\"\"Split documents into passages\"\"\"\n",
    "    titles, texts = [], []\n",
    "    for title, text in zip(documents[\"title\"], documents[\"text\"]):\n",
    "        if text is not None:\n",
    "            for passage in split_text(text):\n",
    "                titles.append(title if title is not None else \"\")\n",
    "                texts.append(passage)\n",
    "    return {\"title\": titles, \"text\": texts}\n",
    "\n",
    "\n",
    "def embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast) -> dict:\n",
    "    \"\"\"Compute the DPR embeddings of document passages\"\"\"\n",
    "    input_ids = ctx_tokenizer(\n",
    "        documents[\"title\"], documents[\"text\"], truncation=True, padding=\"longest\", return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    "    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n",
    "    # return embeddings.detach().cpu().numpy()\n",
    "    return {\"embeddings\": embeddings.detach().cpu().numpy().flatten()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "print(Sequence(Value(\"float32\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay\n",
      "boo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2/2 [00:00<00:00, 124.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "assert os.path.isfile(\"my_knowledge_dataset.csv\"), \"Please provide a valid path to a csv file\"\n",
    "\n",
    "# You can load a Dataset object this way\n",
    "dataset = load_dataset(\n",
    "    \"csv\", data_files=[\"my_knowledge_dataset.csv\"], split=\"train\", delimiter=\", \", column_names=[\"title\", \"text\"]\n",
    ")\n",
    "\n",
    "# More info about loading csv files in the documentation: https://huggingface.co/docs/datasets/loading_datasets?highlight=csv#csv-files\n",
    "\n",
    "# Then split the documents into passages of 100 words\n",
    "# dataset = dataset.map(split_documents, batched=True, num_proc=1)\n",
    "\n",
    "# And compute the embeddings\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device=device)\n",
    "ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "new_features = Features(\n",
    "    {\"title\": Value(\"string\"), \"text\": Value(\"string\"), \"embeddings\": Sequence(Value(\"float32\"))}\n",
    ")  # optional, save as float32 instead of float64 to save space\n",
    "print (\"yay\")\n",
    "dataset = dataset.map(\n",
    "    partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer),\n",
    "    batched=False,\n",
    "    batch_size=1,\n",
    "    features=new_features,\n",
    ")\n",
    "print (\"boo\")\n",
    "\n",
    "# And finally save your dataset\n",
    "# passages_path = os.path.join(\"my_knowledge_dataset.csv\".output_dir, \"my_knowledge_dataset\")\n",
    "dataset.save_to_disk(\"test2\")\n",
    "# from datasets import load_from_disk\n",
    "# dataset = load_from_disk(passages_path)  # to reload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'title', 'embeddings'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 84.88it/s]\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexHNSWFlat(768, 16, faiss.METRIC_INNER_PRODUCT)\n",
    "dataset.add_faiss_index(\"embeddings\", custom_index=index)\n",
    "\n",
    "# And save the index\n",
    "# index_path = os.path.join(rag_example_args.output_dir, \"my_knowledge_dataset_hnsw_index.faiss\")\n",
    "dataset.get_index(\"embeddings\").save(\"index_path\")\n",
    "# dataset.load_faiss_index(\"embeddings\", index_path)  # to reload the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Chloe\\.cache\\huggingface\\hub\\models--facebook--rag-token-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# Easy way to load the model\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-base\", index_name=\"custom\", indexed_dataset=dataset\n",
    ")\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "\n",
    "# For distributed fine-tuning you'll need to provide the paths instead, as the dataset and the index are loaded separately.\n",
    "# retriever = RagRetriever.from_pretrained(rag_model_name, index_name=\"custom\", passages_path=passages_path, index_path=index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Is Bob nice?\n",
      "A:  Marino simplerwho eveningswho cheerswho eveningswho cheers Lucky Lucky rival rival rival rival rival rival\n"
     ]
    }
   ],
   "source": [
    "question = \"Is Bob nice?\"\n",
    "input_ids = tokenizer.question_encoder(question, return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated = model.generate(input_ids)\n",
    "generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "print (\"Q: \" + question)\n",
    "print (\"A: \" + generated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
