{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chloe\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import List, Optional\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from datasets import Features, Sequence, Value, load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    DPRContextEncoder,\n",
    "    DPRContextEncoderTokenizerFast,\n",
    "    HfArgumentParser,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    RagTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def split_text(text: str, n=100, character=\" \") -> List[str]:\n",
    "    \"\"\"Split the text every ``n``-th occurrence of ``character``\"\"\"\n",
    "    text = text.split(character)\n",
    "    return [character.join(text[i : i + n]).strip() for i in range(0, len(text), n)]\n",
    "\n",
    "\n",
    "def split_documents(documents: dict) -> dict:\n",
    "    \"\"\"Split documents into passages\"\"\"\n",
    "    titles, texts = [], []\n",
    "    for title, text in zip(documents[\"title\"], documents[\"text\"]):\n",
    "        if text is not None:\n",
    "            for passage in split_text(text):\n",
    "                titles.append(title if title is not None else \"\")\n",
    "                texts.append(passage)\n",
    "    return {\"title\": titles, \"text\": texts}\n",
    "\n",
    "\n",
    "def embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast) -> dict:\n",
    "    \"\"\"Compute the DPR embeddings of document passages\"\"\"\n",
    "    input_ids = ctx_tokenizer(\n",
    "        documents[\"title\"], documents[\"text\"], truncation=True, padding=\"longest\", return_tensors=\"pt\", max_length = 512\n",
    "    )[\"input_ids\"]\n",
    "    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n",
    "    # return embeddings.detach().cpu().numpy()\n",
    "    return {\"embeddings\": embeddings.detach().cpu().numpy().flatten()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'text'],\n",
       "    num_rows: 249466\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert os.path.isfile(path), \"Please provide a valid path to a csv file\"\n",
    "\n",
    "# You can load a Dataset object this way\n",
    "dataset = load_dataset(\n",
    "    \"csv\", data_files=[path], delimiter=\",\"\n",
    ")\n",
    "\n",
    "# More info about loading csv files in the documentation: https://huggingface.co/docs/datasets/loading_datasets?highlight=csv#csv-files\n",
    "\n",
    "# Then split the documents into passages of 100 words\n",
    "# dataset = dataset.map(split_documents, batched=True, num_proc=1)\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Klutch 15-Slot Universal Wrench Pouch', 'text': 'description: Sturdy Klutch fabric pouch is ideal for organizing, storing and transporting wrenches. Includes 15 slots that provide ample room for SAE and metric wrenches. Capacity qty. 15, Mounting Type Drawer, hanging, Storage Type Pouch. 15 slots for wrenches Tie-trap design keeps things stored neatly Rolls up to save space Eyelets for mounting Pouch is 17.5in.H x 24.75in.W Slot dimensions smallest 1 1/4in. and largest 2 5/8in. Model Number: 81684. Age Group: Adult.; url: northerntool.com; retailer: Northern Tool; brand: Klutch'}\n",
      "{'title': 'TAG OFF Skin Natural Skin Tag Remover Take Skin Tag Away', 'text': 'description: Tag OFF \"Skin Tag Remover\" is a topical remedy made from all-natural plant extracts that help eliminate those harmless skin overgrowths without any pain. Tag Off removes skin tags the all-natural way. Its main ingredients are Thuja occidentalis and Royal Honey. TAGOFF isproven, safe for all types of skin, formulated in the USA. Main benefits are Chemical free, Noscars, painless, fast, Formula 100% Natural, No side effects.; url: overstock.com; retailer: Overstock.com; brand: Other'}\n",
      "{'title': 'Harley-Davidson Skull LED Fuel Gauge', 'text': 'description: This low-profile fuel gauge replaces the analog readout with bright LED lamps. The six edge-mounted LED lamps glow blue when the tank is full, and extinguish one by one as fuel is consumed.; url: harley-davidson.com; retailer: Harley-Davidson; brand: HARLEY-DAVIDSON'}\n",
      "{'title': 'CHANEL Frt Pocket Handbag Quilted Patent Leather Shoulder Bag, Black, One Size', 'text': 'description: Manufacturer: Chanel Material: Black Quilted Patent Leather Interior Lining: Neon Yellow Fabric Exterior Pockets: One Front Flap Pocket Closes With Cc Turnlock And One Back Patch Pocket Closes With Magnetic Snap Interior Pockets: One Zipper Pocket And Two Patch Pockets Handles: Double Chain And Leather Entwined Shoulder Straps Closure/opening: Top Zipper Closure Hardware: Silver Tone Includes: Original Dust Bag, Authenticity Card And Booklets Origin: Italy Date/ Authenticity Code: 12199646 Retail: $4600 Measurements: 7In High X 13In Wide X 6 1/2In Deep X 10 1/2In Strap Drop Exterior Condition: Like New, Leather Is Clean And Beautiful Interior Condition: Gently Used,'}\n",
      "{'title': 'CHANEL Frt Pocket Handbag Quilted Patent Leather Shoulder Bag, Black, One Size', 'text': 'Has A Few Slight Markings But No Odors Detail: Go In Style With A Gorgeous Chanel Black Quilted Patent Leather Shoulder Bag, This Is A Runway Piece From Their Limited Edition Collection. An Elegant And Timeless Piece To Add To Any Collection. Don\\'t Miss The Opportunity To Own This \"hard To Find\" Bag. Please View The Photos For Best Description Of This Handbag. Be Sure To Browse My Store For More Items You May Love - CHANEL Frt Pocket Handbag Black Quilted Patent Leather Shoulder Bag; url: tradesy.com; retailer: Tradesy; brand: Chanel'}\n",
      "{'title': 'Dell WD15 Monitor Dock 4K with 180W Adapter, USB-C, (450-AEUO, 7FJ4J, 4W2HW)', 'text': 'description: 3x SuperSpeed USB 3.0; 2x USB 2.0. Designed For Dell Latitude 3379, 3390 2-in-1, 3490, 3590, 5280, 5285, 5289, 5290, 5290 2-in-1, 5480, 5490, 5491, 5495, 5580, 5590, 5591, 7280, 7285, 7290, 7380, 7389, 7390, 7390 2-in-1, 7480, 7490; Precision 3520, 3530, 5520, 5530, 5530 2 in 1, 7520, 7530, 7720, 7730; XPS 9360, 9365, 9370, 9560, 9570, 9575 Display / Video: 1x HDMI; 1x VGA; 1x Mini DisplayPort. Networking Data Link Protocol : Gigabit Ethernet 1x RJ-45 Ethernet port; 1x Headphone/Mic 3.5 Millimetre port; 1x audio out 3.5 Millimetre port 180W AC Power Adapter with 7.4 Millimetre barrel'}\n",
      "{'title': 'Dell WD15 Monitor Dock 4K with 180W Adapter, USB-C, (450-AEUO, 7FJ4J, 4W2HW)', 'text': 'Display Port over USB Type C Cable, See compatible systems in the description.Max Resolution:3840 x 2160 @ 30 Hertz, 2560 x 1600 @ 60Hertz Dell WD15 Monitor Dock 4K with 180W Adapter, USB-C, (450-AEUO, 7FJ4J, 4W2HW); url: amazon.com; retailer: Amazon.com; brand: Dell'}\n",
      "{'title': 'Entwined Halo Diamond Engagement Ring - 14K Rose Gold', 'text': 'description: Entwined Halo Diamond Engagement Ring - 14K Rose Gold. This dazzling ring features twisted vines of scalloped pav√© diamonds leading to a delicately recessed halo around the center gem. Stunning diamond accents flow down the gallery for added brilliance (a; url: brilliantearth.com; retailer: Brilliant Earth; brand: Brilliant Earth'}\n",
      "{'title': 'Emser Tile T06FONT0404UT Trav Fontane Tumbled - 4\" x 4\" Square Floor and Wall Ti Ivory Classic Flooring Field Tile', 'text': 'description: Emser Tile, T06FONT0404UT, Tile, Trav Fontane Tumbled, Flooring, Field Tile, Please review Build.com return policy for Flooring and Tile products, certain restrictions may apply on general returns. If flooring arrives damaged or is defective, please call for assistance (800-375-3403) Features: Made from travertine with a honed travertine visual Medium shade variation gives the tile a slightly varied appearance Manufactured in Turkey Covered under a 1 year limited warranty Installation: Designed for multiple applications, including floors and walls This tile can be installed using thin-set Specifications: Width: 3.94 Length: 3.94 Thickness: 0.38 Sheet width: 4 Sheet length: 4 Sheet size'}\n",
      "{'title': 'Emser Tile T06FONT0404UT Trav Fontane Tumbled - 4\" x 4\" Square Floor and Wall Ti Ivory Classic Flooring Field Tile', 'text': 'in square feet: 0.11 sq. ft.; url: build.com; retailer: Build; brand: Emser Tile'}\n",
      "{'title': 'Operation Gridlock Caution Sign Adult Unisex Hoodie', 'text': \"description: Everyone needs a cozy go-to hoodie to curl up in, so go for one that's soft, smooth, and stylish. It's the perfect choice for cooler evenings!* 50% cotton, 50% polyester * Double-lined hood* Double-needle stitching throughout * Air-jet spun yarn with a soft feel and reduced pilling* 1x1 athletic rib knit cuffs and waistband with spandex* Front pouch pocket; url: etsy.com; retailer: Etsy; brand: LibertyTShirts\"}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for row in dataset['train']:\n",
    "    text = row[\"text\"]\n",
    "    print (row)\n",
    "    if count==10:\n",
    "        break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 249466/249466 [14:51<00:00, 279.86 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 249466/249466 [00:00<00:00, 612808.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# And compute the embeddings\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device=device)\n",
    "ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "new_features = Features(\n",
    "    {\"title\": Value(\"string\"), \"text\": Value(\"string\"), \"embeddings\": Sequence(Value(\"float32\"))}\n",
    ")  # optional, save as float32 instead of float64 to save space\n",
    "dataset = dataset.map(\n",
    "    partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer),\n",
    "    batched=False,\n",
    "    batch_size=1,\n",
    "    features=new_features,\n",
    ")\n",
    "\n",
    "# And finally save your dataset\n",
    "# passages_path = os.path.join(path.output_dir, \"my_knowledge_dataset\")\n",
    "dataset.save_to_disk(\"test3\")\n",
    "# from datasets import load_from_disk\n",
    "# dataset = load_from_disk(\"test3\")  # to reload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:13<00:00, 18.59it/s]\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexHNSWFlat(768, 16, faiss.METRIC_INNER_PRODUCT)\n",
    "dataset.add_faiss_index(\"embeddings\", custom_index=index)\n",
    "\n",
    "# And save the index\n",
    "# index_path = os.path.join(rag_example_args.output_dir, \"my_knowledge_dataset_hnsw_index.faiss\")\n",
    "dataset.get_index(\"embeddings\").save(\"index_path\")\n",
    "# dataset.load_faiss_index(\"embeddings\", index_path)  # to reload the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# Easy way to load the model\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-base\", index_name=\"custom\", indexed_dataset=dataset\n",
    ")\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "\n",
    "# For distributed fine-tuning you'll need to provide the paths instead, as the dataset and the index are loaded separately.\n",
    "# retriever = RagRetriever.from_pretrained(rag_model_name, index_name=\"custom\", passages_path=passages_path, index_path=index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is a good phone model?\n",
      "A:  XII Talksoccup laughingAUTEnoughResponse position position position position position position position position position position virus\n"
     ]
    }
   ],
   "source": [
    "question = \"What is a good phone model?\"\n",
    "input_ids = tokenizer.question_encoder(question, return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated = model.generate(input_ids)\n",
    "generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "print (\"Q: \" + question)\n",
    "print (\"A: \" + generated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
